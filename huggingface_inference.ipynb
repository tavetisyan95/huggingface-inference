{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ea78e6-f177-40d6-9f08-902bcda429fa",
   "metadata": {},
   "source": [
    "## Performing inference with the inference API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fc54a3-e103-4b07-9832-7cadcb203099",
   "metadata": {},
   "source": [
    "### Building a query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e84458-99c4-4ca9-a514-63922e3a4208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Defining a query function\n",
    "def query(payload, model_id, api_token):\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    API_URL = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85967771-89b5-4bb8-b0c1-b901e2a8a419",
   "metadata": {},
   "source": [
    "### Single-question requests with the Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b42b40a7-3949-4948-b242-cc98b749e5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9129266142845154, 'start': 9, 'end': 15, 'answer': 'Monday'}\n"
     ]
    }
   ],
   "source": [
    "# Example of a single-question query\n",
    "model_id = \"deepset/roberta-base-squad2\"\n",
    "data = {\"question\": \"What day is it?\",\n",
    "        \"context\": \"Today is Monday.\"}\n",
    "api_token = \"api_OqtjUqliPLylbsHwaRvZhbgylcRZroFofO\"\n",
    "\n",
    "print(query(data, model_id, api_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c127ef4-fcfb-46a9-b5f4-cc8d5497544a",
   "metadata": {},
   "source": [
    "### Batch inference with the Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e7c9ee-6706-4803-8e77-5dc495b6f569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9129266142845154, 'start': 9, 'end': 15, 'answer': 'Monday'}, {'score': 0.9501503705978394, 'start': 31, 'end': 35, 'answer': '1980'}]\n"
     ]
    }
   ],
   "source": [
    "# Example of a batch query\n",
    "data = [{\"question\": \"What day is it?\",\n",
    "         \"context\": \"Today is Monday.\"},\n",
    "        {\"question\": \"When was I born?\",\n",
    "         \"context\": \"I was born in Detroit, USA, in 1980.\"}\n",
    "       ]\n",
    "\n",
    "print(query(data, model_id, api_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76abb208-5685-4f7c-bc7f-b8e3ea1b1c84",
   "metadata": {},
   "source": [
    "### Performing invalid requests with the Inference API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167352c6-dcb4-45cd-92d5-826bee0b3ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'unknown error', 'warnings': [\"'You need to provide a dictionary with keys {question:..., context:...}'\"]}\n"
     ]
    }
   ],
   "source": [
    "# Example of an invalid query\n",
    "data = [{\"question\": \"What day is it?\",\n",
    "         \"text\": \"Today is Monday.\"},\n",
    "        {\"question\": \"When was I born?\",\n",
    "         \"text\": \"I was born in Detroit, USA, in 1980.\"}\n",
    "       ]\n",
    "\n",
    "print(query(data, model_id, api_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb72c3e-fde7-40d1-a505-ccf6ecb2f3b3",
   "metadata": {},
   "source": [
    "## Pipeline vs direct model use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850cb7b-4666-4155-aedc-923bc071c939",
   "metadata": {},
   "source": [
    "### Using transformers pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2eeb54f7-1b64-40b2-8ec4-69c9aa78793b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Instantiating a Transformers pipeline\n",
    "qa_pipeline = pipeline(task=\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7c9822-93d3-4d54-a210-6eacf49aa627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing inference with a pipeline\n",
    "data = [{\"question\": \"What day is it?\",\n",
    "         \"context\": \"Today is Monday.\"},\n",
    "        {\"question\": \"When was I born?\",\n",
    "         \"context\": \"I was born in Detroit, USA, in 1980.\"}\n",
    "       ]\n",
    "\n",
    "result = qa_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a255027-b876-40b3-a681-6a20ec35669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.9574882984161377, 'start': 9, 'end': 15, 'answer': 'Monday'}, {'score': 0.9639834761619568, 'start': 31, 'end': 35, 'answer': '1980'}]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a402f-d59f-4133-adb2-d9fa0401a238",
   "metadata": {},
   "source": [
    "### Using pretrained Transformers models directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb3c12-e3c8-45d6-8a68-8114b9394ad8",
   "metadata": {},
   "source": [
    "#### Getting started with direct model use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ec7e540-d350-45ae-883d-5713a3f4d4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForQuestionAnswering: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
    "\n",
    "# Instantiating our tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\", from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b433a-139c-44cc-8f4b-140c0d59c64d",
   "metadata": {},
   "source": [
    "#### Tokenizing question-context pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3087137d-a023-4a1d-b572-a3a73563fc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the input question and context as a TF tensor\n",
    "inputs = tokenizer(text=\"When was I born?\", \n",
    "                   text_pair=\"I was born in Detroit, USA, in 1980.\", \n",
    "                   add_special_tokens=True, return_tensors=\"tf\")\n",
    "\n",
    "# Extracting the tokens corresponding to the input sequence\n",
    "input_ids = inputs[\"input_ids\"].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df694566-6fce-40f7-b2a1-808bddffd10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
      "array([[   0, 1779,   21,   38, 2421,  116,    2,    2,  100,   21, 2421,\n",
      "          11, 2921,    6, 2805,    6,   11, 5114,    4,    2]])>, 'attention_mask': <tf.Tensor: shape=(1, 20), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>}\n"
     ]
    }
   ],
   "source": [
    "# Examining the model inputs\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cb48069-15ff-4179-8f72-04049f59286f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0 1779   21   38 2421  116    2    2  100   21 2421   11 2921    6\n",
      " 2805    6   11 5114    4    2]\n"
     ]
    }
   ],
   "source": [
    "# Examining input IDs\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0c3697e-1f98-4b4a-af89-a81f229eeeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>When was I born?</s></s>I was born in Detroit, USA, in 1980.</s>\n"
     ]
    }
   ],
   "source": [
    "# Converting the encoded sequence back into a string\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c61da8-3cb4-4511-b453-a27a4f239a76",
   "metadata": {},
   "source": [
    "#### Performing single-sample inference with direct model use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89eda53f-db7c-4ae4-bc1e-e9458f59ca23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFQuestionAnsweringModelOutput(loss=None, start_logits=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 1.2259467 , -7.791437  , -8.260778  , -8.249651  , -7.5693817 ,\n",
      "        -7.936594  , -8.363062  , -7.1329803 , -0.18496554, -2.9790633 ,\n",
      "        -1.8434091 , -1.3693225 ,  0.71017456, -5.1979237 , -1.2315122 ,\n",
      "        -4.5270185 ,  0.62987506,  6.598527  , -1.2783077 , -7.465865  ]],\n",
      "      dtype=float32)>, end_logits=<tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
      "array([[ 1.6835405 , -6.918748  , -7.306207  , -7.5368285 , -6.0764575 ,\n",
      "        -3.6980824 , -5.6374154 , -4.4059825 , -4.577457  , -5.964948  ,\n",
      "        -2.6135447 , -5.8583727 , -0.7394024 , -5.670526  ,  0.16733687,\n",
      "        -2.0341313 , -2.189781  ,  7.048407  ,  3.6379344 , -4.67701   ]],\n",
      "      dtype=float32)>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Passing the tokenized inputs to the model\n",
    "answer = model(inputs)\n",
    "\n",
    "# Examining the model output\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c98267b-f924-4180-a87f-87e2b22a39c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the start and end logits from the output\n",
    "answer_start_scores = answer.start_logits\n",
    "answer_end_scores = answer.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96d62979-ebc0-4a2e-9b0c-d2576dd16584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Finding the start and end indices with the highest score\n",
    "answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0]\n",
    "answer_end = tf.argmax(answer_end_scores, axis=1).numpy()[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42e37cc6-ec46-4956-901a-58d36a0c4b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer start index: 17\n",
      "Answer end index: 18\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer start index: {answer_start}\")\n",
    "print(f\"Answer end index: {answer_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "245a35f1-b6cd-4666-aa86-6eac6d68ccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to the question:  1980\n"
     ]
    }
   ],
   "source": [
    "# Converting the ID back to a string\n",
    "final_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "# Viewing the answer\n",
    "print(f\"Answer to the question: {final_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39def8c6-d8d7-404b-8d7f-2d860c27d6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer start index: 12\n",
      "Answer end index: 15\n",
      "Answer to the question:  Detroit, USA\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the input question and context as a TF tensor\n",
    "inputs = tokenizer(\"Where was I born?\", \n",
    "                   \"I was born in Detroit, USA, in 1980.\", \n",
    "                   add_special_tokens=True, return_tensors=\"tf\")\n",
    "\n",
    "answer = model(inputs)\n",
    "\n",
    "# Extracting the tokens corresponding to the input sequence\n",
    "input_ids = inputs[\"input_ids\"].numpy()[0]\n",
    "\n",
    "answer_start_scores = answer.start_logits\n",
    "answer_end_scores = answer.end_logits\n",
    "\n",
    "# Finding the start and end indices with the highest score\n",
    "answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0]\n",
    "answer_end = tf.argmax(answer_end_scores, axis=1).numpy()[0] + 1\n",
    "\n",
    "print(f\"Answer start index: {answer_start}\")\n",
    "print(f\"Answer end index: {answer_end}\")\n",
    "\n",
    "# Converting the ID back to a string\n",
    "final_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "# Viewing the answer\n",
    "print(f\"Answer to the question: {final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7bb30-e599-4c8c-9f2d-113183046f23",
   "metadata": {},
   "source": [
    "#### Performing batch inference with direct model use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e130e2c9-1853-44f2-a6dd-c86149204e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where was I born?\n",
      "Context: I was born in Detroit, USA, in 1980.\n",
      "Answer start index: 12\n",
      "Answer end index: 15\n",
      "Answer to the question:  Detroit, USA\n",
      "\n",
      "Question: What day is it?\n",
      "Context: Today is Monday.\n",
      "Answer start index: 10\n",
      "Answer end index: 11\n",
      "Answer to the question:  Monday\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\"Where was I born?\", \"What day is it?\"]\n",
    "contexts = [\"I was born in Detroit, USA, in 1980.\", \"Today is Monday.\"]\n",
    "\n",
    "# Iterating over the questions\n",
    "for question, context in zip(questions, contexts):\n",
    "    # Viewing the current question and context\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Context: {context}\")\n",
    "    \n",
    "    # Tokenizing the input question and context as a TF tensor\n",
    "    inputs = tokenizer(question, context,\n",
    "                       add_special_tokens=True, return_tensors=\"tf\")\n",
    "\n",
    "    answer = model(inputs)\n",
    "\n",
    "    # Extracting the tokens corresponding to the input sequence\n",
    "    input_ids = inputs[\"input_ids\"].numpy()[0]\n",
    "\n",
    "    answer_start_scores = answer.start_logits\n",
    "    answer_end_scores = answer.end_logits\n",
    "\n",
    "    # Finding the start and end indices with the highest score\n",
    "    answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0]\n",
    "    answer_end = tf.argmax(answer_end_scores, axis=1).numpy()[0] + 1\n",
    "\n",
    "    print(f\"Answer start index: {answer_start}\")\n",
    "    print(f\"Answer end index: {answer_end}\")\n",
    "\n",
    "    # Converting the ID back to a string\n",
    "    final_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    # Viewing the answer\n",
    "    print(f\"Answer to the question: {final_answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "714c7cd1-1e59-4032-b619-c5329c2f9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing and padding input sequences\n",
    "inputs = tokenizer(questions, contexts, \n",
    "                   add_special_tokens=True, return_tensors=\"tf\", \n",
    "                   padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b309921f-a8d9-47c9-8e93-5b1cd20ec083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing the inputs to our model\n",
    "answers = model(inputs)\n",
    "\n",
    "# Extracting the tokens corresponding to the input sequences\n",
    "input_ids = inputs[\"input_ids\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0de8d9fb-86e7-4c86-ae41-0bb7a4818ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0 13841    21    38  2421   116     2     2   100    21  2421    11\n",
      "   2921     6  2805     6    11  5114     4     2]\n",
      " [    0  2264   183    16    24   116     2     2  5625    16   302     4\n",
      "      2     1     1     1     1     1     1     1]]\n"
     ]
    }
   ],
   "source": [
    "# Viewing our input IDs\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6bb8107-8d71-47a2-9d56-d028f0d33d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>What day is it?</s></s>Today is Monday.</s><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "# Viewing one of the input ID arrays\n",
    "print(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a96d847-6cb0-42bf-a57b-4301849662a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFQuestionAnsweringModelOutput(loss=None, start_logits=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 0.78155375, -8.073274  , -8.34409   , -8.468487  , -8.099401  ,\n",
      "        -8.646169  , -8.6963625 , -7.6574035 , -0.64569664, -3.0957458 ,\n",
      "        -2.2456067 ,  0.14127505,  6.086786  , -3.696553  , -0.37048122,\n",
      "        -5.0749235 , -2.5687819 ,  0.3333048 , -3.133765  , -8.028114  ],\n",
      "       [ 1.7058821 , -8.217684  , -8.914576  , -8.764219  , -8.871216  ,\n",
      "        -9.367665  , -9.4340925 , -8.613971  , -0.7869898 , -5.4473844 ,\n",
      "         5.1352215 , -4.5842705 , -8.896799  , -9.427734  , -9.427734  ,\n",
      "        -9.427734  , -9.427734  , -9.427734  , -9.427734  , -9.427734  ]],\n",
      "      dtype=float32)>, end_logits=<tf.Tensor: shape=(2, 20), dtype=float32, numpy=\n",
      "array([[ 1.0022359 , -7.5837913 , -7.652511  , -7.7337627 , -6.577329  ,\n",
      "        -4.721192  , -6.1436515 , -4.644155  , -5.339603  , -6.4877644 ,\n",
      "        -3.6098654 , -3.8038702 ,  5.0979486 , -1.4663633 ,  5.515522  ,\n",
      "         1.7955147 , -4.917917  ,  2.0246046 ,  2.1283152 , -5.46772   ],\n",
      "       [ 1.9663267 , -7.624847  , -5.9132714 , -7.2117314 , -7.2619796 ,\n",
      "        -6.2353854 , -6.845627  , -7.1807027 , -3.6652987 , -6.841409  ,\n",
      "         4.9925647 ,  0.33412167, -7.2172003 , -8.610821  , -8.610821  ,\n",
      "        -8.610821  , -8.610821  , -8.610821  , -8.610821  , -8.610821  ]],\n",
      "      dtype=float32)>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Viewing the model output\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8be1852e-e76d-4fd1-8918-ff6c628a361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the start and end logits from the output\n",
    "answer_start_scores = answers.start_logits\n",
    "answer_end_scores = answers.end_logits\n",
    "\n",
    "# Finding the start and end indices with the highest score\n",
    "answer_starts = tf.argmax(answer_start_scores, axis=1).numpy()\n",
    "answer_ends = tf.argmax(answer_end_scores, axis=1).numpy() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abb57b59-10d6-4ee7-a782-f32320d19c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer start indices: [12 10]\n",
      "Answer end indices: [15 11]\n"
     ]
    }
   ],
   "source": [
    "# Viewing answer start and end indices\n",
    "print(f\"Answer start indices: {answer_starts}\")\n",
    "print(f\"Answer end indices: {answer_ends}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "65ebf5fe-6e18-4d66-be85-a4e497b614c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the IDs back to string form\n",
    "final_answers = []\n",
    "\n",
    "for i, starts_ends in enumerate(zip(answer_starts, answer_ends)):\n",
    "    final_answers.append(tokenizer.\n",
    "                         convert_tokens_to_string(\n",
    "                             tokenizer.convert_ids_to_tokens(\n",
    "                                 input_ids[i, starts_ends[0]:starts_ends[1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea8f1219-615c-45af-905c-1330426afbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Detroit, USA', ' Monday']\n"
     ]
    }
   ],
   "source": [
    "# Viewing our answers\n",
    "print(final_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2d49331-c11f-40af-9512-d1f98ad03b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: Where was I born?\n",
      "Context 1: I was born in Detroit, USA, in 1980.\n",
      "Answer start index: 12\n",
      "Answer end index: 15\n",
      "Answer 1:  Detroit, USA\n",
      "\n",
      "Question 2: What day is it?\n",
      "Context 2: Today is Monday.\n",
      "Answer start index: 10\n",
      "Answer end index: 11\n",
      "Answer 2:  Monday\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing our questions and contexts with their respective answers\n",
    "for i, answer in enumerate(final_answers):\n",
    "    print(f\"Question {i + 1}: {questions[i]}\")\n",
    "    print(f\"Context {i + 1}: {contexts[i]}\")\n",
    "    print(f\"Answer start index: {answer_starts[i]}\")\n",
    "    print(f\"Answer end index: {answer_ends[i]}\")\n",
    "    print(f\"Answer {i + 1}: {answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
